<div align="center">
  <img src="ollama-nextjs-ui.gif">
</div>

<h1 align="center">
  Vollst√§ndige & wundersch√∂ne Web-Oberfl√§che f√ºr Ollama LLMs
</h1>

<div align="center">
  
![GitHub Repo stars](https://img.shields.io/github/stars/jakobhoeg/nextjs-ollama-llm-ui)
  
</div>


Starten Sie **schnell**, **lokal** und sogar **offline** mit gro√üen Sprachmodellen. Dieses Projekt soll der einfachste Weg sein, um mit LLMs zu beginnen. Keine m√ºhsamen und nervigen Setups erforderlich!

# Funktionen ‚ú®

- **Sch√∂ne & intuitive Benutzeroberfl√§che:** Inspiriert von ChatGPT, um die Benutzererfahrung zu verbessern.
- **Vollst√§ndig lokal:** Speichert Chats im Localstorage f√ºr Bequemlichkeit. Keine Datenbank erforderlich.
- **Vollst√§ndig responsiv:** Nutzen Sie Ihr Handy zum Chatten, genauso bequem wie auf dem Desktop.
- **Einfache Einrichtung:** Keine m√ºhsame und nervige Einrichtung erforderlich. Klonen Sie einfach das Repository und legen Sie los!
- **Code-Syntax-Hervorhebung:** Nachrichten, die Code enthalten, werden zur leichteren Lesbarkeit hervorgehoben.
- **Codebl√∂cke einfach kopieren:** Kopieren Sie die hervorgehobenen Codes mit nur einem Klick.
- **Modelle herunterladen/ziehen & l√∂schen:** Laden Sie Modelle direkt √ºber die Benutzeroberfl√§che herunter und l√∂schen Sie sie.
- **Zwischen Modellen wechseln:** Wechseln Sie schnell zwischen Modellen mit einem Klick.
- **Chat-Verlauf:** Chats werden gespeichert und sind leicht zug√§nglich.
- **Hell- & Dunkelmodus:** Wechseln Sie zwischen Hell- und Dunkelmodus.

# Vorschau

https://github.com/jakobhoeg/nextjs-ollama-llm-ui/assets/114422072/08eaed4f-9deb-4e1b-b87a-ba17d81b9a02

# Anforderungen ‚öôÔ∏è

Um die Web-Oberfl√§che zu verwenden, m√ºssen diese Anforderungen erf√ºllt sein:

1. Laden Sie [Ollama](https://ollama.com/download) herunter und f√ºhren Sie es aus. Alternativ k√∂nnen Sie es in einem Docker-Container ausf√ºhren. √úberpr√ºfen Sie die [Dokumentation](https://github.com/ollama/ollama) f√ºr Anweisungen.
2. Node.js (18+) und npm sind erforderlich. [Herunterladen](https://nodejs.org/en/download)

# Deployen Sie Ihre eigene Instanz mit einem Klick auf Vercel oder Netlify ‚ú®

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fjakobhoeg%2Fnextjs-ollama-llm-ui&env=NEXT_PUBLIC_OLLAMA_URL&envDescription=Your%20Ollama%20URL) [![Deploy to Netlify Button](https://www.netlify.com/img/deploy/button.svg)](https://app.netlify.com/start/deploy?repository=https://github.com/jakobhoeg/nextjs-ollama-llm-ui)

Sie m√ºssen Ihre [OLLAMA_ORIGINS](https://github.com/ollama/ollama/blob/main/docs/faq.md) Umgebungsvariable auf dem Computer festlegen, auf dem Ollama l√§uft:

```
OLLAMA_ORIGINS="https://your-app.vercel.app/"
```

# Installation üìñ

[![Packaging status](https://repology.org/badge/vertical-allrepos/nextjs-ollama-llm-ui.svg?columns=3)](https://repology.org/project/nextjs-ollama-llm-ui/versions)

Verwenden Sie ein vorgefertigtes Paket eines der unterst√ºtzten Paketmanager, um eine lokale Umgebung der Web-Oberfl√§che auszuf√ºhren. Alternativ k√∂nnen Sie auch aus dem Quellcode installieren, wie in den folgenden Anweisungen beschrieben.

> [!HINWEIS]  
> Wenn Ihre Frontend-Anwendung auf etwas anderem als `http://localhost` oder `http://127.0.0.1` l√§uft, m√ºssen Sie die OLLAMA_ORIGINS auf Ihre Frontend-URL einstellen.
>
> Dies wird auch in der [Dokumentation](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server) angegeben:
> 
> `Ollama erlaubt standardm√§√üig Cross-Origin-Anfragen von 127.0.0.1 und 0.0.0.0. Weitere Urspr√ºnge k√∂nnen mit OLLAMA_ORIGINS konfiguriert werden.`

## Installation aus dem Quellcode

**1. Klonen Sie das Repository in ein Verzeichnis auf Ihrem PC √ºber die Eingabeaufforderung:**

```
git clone https://github.com/jakobhoeg/nextjs-ollama-llm-ui
```

**2. Wechseln Sie in das Verzeichnis:**

```
cd nextjs-ollama-llm-ui
```

**3. Benennen Sie die `.example.env` in `.env` um:**

```
mv .example.env .env
```

**4. Wenn Ihre Instanz von Ollama NICHT auf der Standard-IP-Adresse und dem Standardport l√§uft, √§ndern Sie die Variable in der .env-Datei, um Ihre Nutzung anzupassen:**

```
NEXT_PUBLIC_OLLAMA_URL="http://localhost:11434"
```

**5. Abh√§ngigkeiten installieren:**

```
npm install
```

**6. Starten Sie den Entwicklungsserver:**

```
npm run dev

npm install && npm run dev

```

**7. Gehen Sie zu [localhost:3000](http://localhost:3000) und beginnen Sie, mit Ihrem bevorzugten Modell zu chatten!**

# Kommende Funktionen

Dies ist eine To-Do-Liste mit bevorstehenden Funktionen.
- ‚úÖ Sprachsteuerung
- ‚úÖ Code-Syntax-Hervorhebung
- ‚¨úÔ∏è M√∂glichkeit, ein Bild in der Eingabeaufforderung zu senden, um Vision-Sprachmodelle zu nutzen.
- ‚¨úÔ∏è M√∂glichkeit zur Regenerierung von Antworten
- ‚¨úÔ∏è Import und Export von Chats

# Technologiestack

[NextJS](https://nextjs.org/) - React Framework f√ºr das Web

[TailwindCSS](https://tailwindcss.com/) - Utility-first CSS Framework

[shadcn-ui](https://ui.shadcn.com/) - UI-Komponente, die mit Radix UI und Tailwind CSS erstellt wurde

[shadcn-chat](https://github.com/jakobhoeg/shadcn-chat) - Chat-Komponenten f√ºr NextJS/React-Projekte

[Framer Motion](https://www.framer.com/motion/) - Bewegungs-/Animationsbibliothek f√ºr React

[Lucide Icons](https://lucide.dev/) - Icon-Bibliothek

# N√ºtzliche Links

[Medium-Artikel](https://medium.com/@bartek.lewicz/launch-your-own-chatgpt-clone-for-free-on-colab-shareable-and-online-in-less-than-10-minutes-da19e44be5eb) - Wie man seinen eigenen ChatGPT-Klon kostenlos auf Google Colab startet. Von Bartek Lewicz.

[Lobehub-Erw√§hnung](https://lobehub.com/blog/5-ollama-web-ui-recommendation#5-next-js-ollama-llm-ui) - F√ºnf ausgezeichnete kostenlose Ollama WebUI-Client-Empfehlungen
```
